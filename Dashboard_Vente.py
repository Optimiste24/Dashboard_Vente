# vente.py
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import streamlit as st
from pathlib import Path

import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

from sklearn.preprocessing import StandardScaler, TargetEncoder
from sklearn.model_selection import train_test_split
from sklearn.multioutput import MultiOutputRegressor
from sklearn.metrics import mean_squared_error


from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import TimeSeriesSplit
import tensorflow as tf



# Configuration initiale
def configure_visuals():
    sns.set_style("whitegrid")
    plt.rc('figure', autolayout=True, figsize=(11, 4))
    plt.rc('axes', labelweight='bold', labelsize='large',
           titleweight='bold', titlesize=16, titlepad=10)

# Configuration de la page
st.set_page_config(
    page_title="Favorita Sales Analytics",
    layout="wide",
    initial_sidebar_state="expanded"
)


# Chargement des donnÃ©es depuis GCS
@st.cache_data
def load_data():
    base_url = "https://storage.googleapis.com/venteequateur/data/"
    
    train_files = [
        "train_2013.csv", "train_2014.csv",
        "train_2015.csv", "train_2016.csv", "train_2017.csv"
    ]
    
    # Chargement des fichiers d'entraÃ®nement
    train_dfs = []
    for file in train_files:
        try:
            df = pd.read_csv(base_url + file, parse_dates=['date'])
            train_dfs.append(df)
        except Exception as e:
            st.error(f"Erreur chargement {file}: {str(e)}")
    
    data = {
        'train': pd.concat(train_dfs, ignore_index=True) if train_dfs else None,
        'stores': pd.read_csv(base_url + 'stores.csv'),
        'oil': pd.read_csv(base_url + 'oil.csv', parse_dates=['date']),
        'holidays': pd.read_csv(base_url + 'holidays_events.csv', parse_dates=['date'])
    }
    return data


def about_page():
    st.title("ğŸ‘¨â€ğŸ’» Ã€ Propos de Moi")
    
    # Section Profil avec colonnes
    col1, col2 = st.columns([0.3, 0.7])
    
    with col1:
        # Lien direct vers l'image 
        st.image("https://storage.googleapis.com/venteequateur/data/Profil_pro.jpg", 
                width=200,
                caption="Yassh Agoro")
    
    with col2:
        st.markdown("""
        **Yassh Agoro**  
        *Data Scientist Consultant | Expert en Analyse PrÃ©dictive*  
        """)
        
        # Badges interactifs
        st.link_button("ğŸ”— LinkedIn", "https://www.linkedin.com/in/yassh-agoro-91a460315")
        st.link_button("ğŸŒŸ Malt", "https://www.malt.fr/profile/yasshagoro1")
    
    # Valeur proposition
    st.header("ğŸ¯ Ma Mission")
    st.markdown("""
    > *Je transforme les donnÃ©es des commerces, franchises et PME en **dÃ©cisions stratÃ©giques et actions concrÃ¨tes**.*  
    > *Combinaison unique d'expertise terrain (vente/gestion) et technique (data science/automatisation).*
    """)
    
    # Services sous forme de cartes
    st.header("ğŸ’¡ Mes Services")
    
    services = [
        ("ğŸ“Š", "**Suivi temps rÃ©el des KPI**", "Streamlit, Power BI, Excel VBA"),
        ("âš™ï¸", "**Automatisation des reportings**", "Gain de temps jusqu'Ã  80%"),
        ("ğŸ”®", "**PrÃ©diction des ventes**", "ModÃ¨les SARIMAX, XGBoost, LSTM"),
        ("ğŸ“ˆ", "**Optimisation rentabilitÃ©**", "Analyse produits/pricing/clients"),
        ("ğŸ–¥ï¸", "**Dashboards stratÃ©giques**", "Visuels simples et actionnables")
    ]
    
    for icon, title, desc in services:
        with st.expander(f"{icon} {title}"):
            st.caption(desc)
    
    # Projet actuel
    st.header("ğŸš€ Projet Actuel")
    st.markdown("""
    **ğŸ“Š PrÃ©diction des ventes quotidiennes par magasin/famille**  
    *Pour une chaÃ®ne de distribution Ã©quatorienne (Dataset: CorporaciÃ³n Favorita)*  
    
    - **Objectif** : Anticiper les demandes avec <90% de prÃ©cision (RMSE)  
    - **Technos** : Python, Streamlit, LightGBM, Prophet  
    - **Livrables** : Dashboard interactif + API de prÃ©diction  
    """)
    
    # CompÃ©tences techniques (avec liens d'images valides)
    st.header("ğŸ›  Stack Technique")
    
    techs = {
        "Python": "https://upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Python-logo-notext.svg/1869px-Python-logo-notext.svg.png",
        "Streamlit": "https://streamlit.io/images/brand/streamlit-mark-color.png",
        "Power BI": "https://upload.wikimedia.org/wikipedia/commons/thumb/c/cf/Power_bi_logo_black.svg/1200px-Power_bi_logo_black.svg.png",
        "SQL": "https://cdn-icons-png.flaticon.com/512/4492/4492311.png"
    }
    
    cols = st.columns(len(techs))
    for col, (name, url) in zip(cols, techs.items()):
        with col:
            st.image(url, width=60)
            st.caption(f"**{name}**")
    
    # Contact
    st.divider()
    st.markdown("""
    âœ‰ï¸ **Contact** : [yagoropro@outlook.fr](mailto:yagoropro@outlook.fr)  
    ğŸ“ **TÃ©lÃ©phone** : +33 9 51 79 59 24  
    """)
def eda_page(data):
    st.title("ğŸ“Š Exploration des DonnÃ©es (EDA)")
    st.markdown("""
    **Objectif:** Identifier les tendances, saisonnalitÃ©s et relations clÃ©s dans les donnÃ©es.
    """)

    # VÃ©rification que les donnÃ©es sont chargÃ©es
    if data is None or 'train' not in data:
        st.error("Erreur: Les donnÃ©es n'ont pas pu Ãªtre chargÃ©es!")
        return

    # Onglets principaux
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“ˆ Ventes", "ğŸª Magasins", "â›½ PÃ©trole", "ğŸ‰ Jours FÃ©riÃ©s"])

    # 1. Analyse des Ventes
    with tab1:
        st.header("Analyse des Ventes Temporelles")
        
        # Convertir la date si nÃ©cessaire
        if not pd.api.types.is_datetime64_any_dtype(data['train']['date']):
            data['train']['date'] = pd.to_datetime(data['train']['date'])
        
        # SÃ©lecteur de pÃ©riode
        col1, col2 = st.columns(2)
        with col1:
            start_date = st.date_input("Date dÃ©but", data['train']['date'].min())
        with col2:
            end_date = st.date_input("Date fin", data['train']['date'].max())
        
        # Filtrage des donnÃ©es
        filtered = data['train'][
            (data['train']['date'] >= pd.to_datetime(start_date)) & 
            (data['train']['date'] <= pd.to_datetime(end_date))
        ]
        
        # AgrÃ©gation interactive
        freq = st.radio("FrÃ©quence", ["Journalier", "Hebdomadaire", "Mensuel"], horizontal=True)
        
        if freq == "Journalier":
            sales = filtered.groupby('date')['sales'].sum()
        elif freq == "Hebdomadaire":
            sales = filtered.set_index('date').resample('W-Mon')['sales'].sum()
        else:
            sales = filtered.set_index('date').resample('ME')['sales'].sum()
        
        # Visualisation avec Plotly
        fig = px.line(sales, title=f"Ventes {freq.lower()}s")
        st.plotly_chart(fig, use_container_width=True)
    
    with tab2:
        st.subheader("Analyse GÃ©ographique des Magasins")
        
        # Fusion avec les donnÃ©es magasins
        merged = pd.merge(data['train'], data['stores'], on='store_nbr')
        
        # SÃ©lection du type d'analyse
        analysis_type = st.selectbox("Vue par:", ["Ville", "Type de Magasin"])
        
        if analysis_type == "Ville":
            fig = px.bar(merged.groupby('city')['sales'].sum().sort_values(ascending=False),
                         title="Ventes Totales par Ville")
        elif analysis_type == "Type de Magasin":
            fig = px.pie(merged.groupby('type')['sales'].sum(), 
                         names=merged['type'].unique(), 
                         title="RÃ©partition des Ventes par Type de Magasin")
#        else:
#            fig = px.box(merged, x='cluster', y='sales', 
#                         title="Distribution des Ventes par Cluster")
        
        st.plotly_chart(fig, use_container_width=True)


    # 2. Analyse PÃ©trole-Ventes (avec corrÃ©lation colorÃ©e)
    with tab3:
        st.header("Analyse Prix du PÃ©trole vs Ventes")
        
        # Fusion des donnÃ©es
        oil_sales = pd.merge(
            data['train'].groupby('date')['sales'].sum().reset_index(),
            data['oil'],
            on='date',
            how='inner'
        ).dropna()
        
        # Calcul de corrÃ©lation
        corr = oil_sales['sales'].corr(oil_sales['dcoilwtico'])
        st.metric("CorrÃ©lation globale", f"{corr:.2f}")
        
        # Graphique avec double axe et gradient de couleur
        fig = go.Figure()
        
        # Ajout des ventes (axe Y gauche)
        fig.add_trace(go.Scatter(
            x=oil_sales['date'],
            y=oil_sales['sales'],
            name="Ventes",
            line=dict(color='royalblue')
        ))
        
        # Ajout du pÃ©trole (axe Y droit)
        fig.add_trace(go.Scatter(
            x=oil_sales['date'],
            y=oil_sales['dcoilwtico'],
            name="Prix du PÃ©trole",
            yaxis="y2",
            line=dict(color='crimson')
        ))
        
        # Mise en forme
        fig.update_layout(
            title="Relation Temporelle: Ventes vs Prix du PÃ©trole",
            yaxis=dict(title="Ventes", side="left"),
            yaxis2=dict(title="Prix du PÃ©trole", overlaying="y", side="right"),
            hovermode="x unified",
            colorway=["#636EFA", "#EF553B"]
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # Heatmap de corrÃ©lation par mois
        oil_sales['month'] = oil_sales['date'].dt.month
        monthly_corr = oil_sales.groupby('month')[['sales', 'dcoilwtico']].corr().iloc[0::2,-1].reset_index()
        
        fig = px.bar(monthly_corr, x='month', y='dcoilwtico', 
                     color='dcoilwtico',
                     color_continuous_scale=px.colors.diverging.RdBu,
                     range_color=[-1, 1],
                     title="CorrÃ©lation par Mois")
        st.plotly_chart(fig, use_container_width=True)

    with tab4:
        st.subheader("Impact des Jours FÃ©riÃ©s")
        # Boxplot ventes vs fÃ©riÃ©s
        holidays = data['holidays'][data['holidays']['locale'] == 'National']
        merged = pd.merge(data['train'], holidays[['date', 'type']], on='date', how='left')
        merged['is_holiday'] = merged['type'].notna()
        st.bar_chart(merged.groupby('is_holiday')['sales'].mean())

def feature_engineering_page(data):
    st.title("ğŸ›  Feature Engineering")
    
    with st.expander("â± Features Temporelles", expanded=True):
        # Exemple de crÃ©ation de features
        if st.button("Ajouter les caractÃ©ristiques temporelles"):
            data['train']['day_of_week'] = data['train']['date'].dt.dayofweek
            data['train']['month'] = data['train']['date'].dt.month
            st.success("Features temporelles ajoutÃ©es !")
    
    with st.expander("ğŸª Features Magasins"):
        if st.button("Ajouter les clusters de magasin"):
            data['train'] = pd.merge(data['train'], data['stores'][['store_nbr', 'cluster']], on='store_nbr')
            st.success("Clusters ajoutÃ©s !")
    
    with st.expander("ğŸ“… Jours SpÃ©ciaux"):
        # Exemple pour le tremblement de terre
        if st.button("Marquer la pÃ©riode post-tremblement de terre"):
            earthquake_date = pd.to_datetime('2016-04-16')
            data['train']['post_earthquake'] = (
                (data['train']['date'] >= earthquake_date) & 
                (data['train']['date'] <= earthquake_date + pd.Timedelta(days=30))
            )
            st.success("PÃ©riode marquÃ©e !")


def preprocessing_page(data):
    st.title("ğŸ§¹ PrÃ©traitement des DonnÃ©es")
    
    if data is None or 'train' not in data:
        st.error("DonnÃ©es non chargÃ©es ou format incorrect!")
        return
    
    with st.expander("ğŸ” DonnÃ©es Brutes", expanded=False):
        st.dataframe(data['train'].head())
    
    # 1. Gestion des DonnÃ©es Manquantes
    with st.expander("ğŸ•³ï¸ Traitement des Valeurs Manquantes", expanded=True):
        st.subheader("Prix du PÃ©trole")
        oil_interp = st.radio("MÃ©thode d'interpolation pour le pÃ©trole:",
                             ["Linear", "Backward Fill"],
                             index=0)
        
        if st.button("Appliquer le traitement"):
            # Interpolation du pÃ©trole
            if oil_interp == "Linear":
                data['oil']['dcoilwtico'] = data['oil']['dcoilwtico'].interpolate()
            else:
                data['oil']['dcoilwtico'] = data['oil']['dcoilwtico'].bfill()
            
            # Remplacer les promotions manquantes par 0
            data['train']['onpromotion'] = data['train']['onpromotion'].fillna(0)
            
            st.success("Traitement appliquÃ© avec succÃ¨s!")
    
    # 2. Encodage des Variables CatÃ©gorielles
    with st.expander("ğŸ”  Encodage des CatÃ©gories"):
        encoding_method = st.selectbox("MÃ©thode d'encodage:",
                                     ["Target Encoding", "Embedding"])
        
        if st.button("Encoder les variables"):
            if encoding_method == "Target Encoding":
                # Target Encoding pour 'family'
                encoder = TargetEncoder()
                data['train']['family_encoded'] = encoder.fit_transform(
                    data['train'][['family']], 
                    data['train']['sales']
                )
            # (L'Embedding nÃ©cessiterait un modÃ¨le neural)
            
            # Encodage simple pour store_nbr (peut Ãªtre amÃ©liorÃ©)
            data['train']['store_encoded'] = data['train']['store_nbr'].astype('category').cat.codes
            
            st.success("Encodage terminÃ©!")
    
    # 3. PrÃ©paration pour MultiOutput (8 semaines)
    with st.expander("ğŸ“… PrÃ©paration des SÃ©ries Temporelles"):
        st.markdown("""
        **StratÃ©gie MultiOutput:**
        - Horizon: 8 semaines
        - Temps d'avance: 1 semaine
        """)

    
    if st.button("PrÃ©parer les donnÃ©es pour la prÃ©diction multi-pÃ©riodes"):
        try:
            prepared_data = prepare_multioutput_data(
                data['train'], 
                n_steps=8, 
                gap=1
            )
            st.session_state.processed_data = prepared_data
            st.success(f"DonnÃ©es prÃ©parÃ©es. Shape: {prepared_data.shape}")
        except Exception as e:
            st.error(f"Erreur lors de la prÃ©paration: {str(e)}")


    # Affichage des donnÃ©es transformÃ©es
    if 'processed_data' in st.session_state:
        with st.expander("ğŸ“¦ DonnÃ©es TransformÃ©es", expanded=True):
            st.dataframe(st.session_state.processed_data.head())

def create_time_features(df):
    """CrÃ©e des features temporelles"""
    df['day_of_week'] = df['date'].dt.dayofweek
    df['day_of_month'] = df['date'].dt.day
    df['week_of_year'] = df['date'].dt.isocalendar().week
    df['month'] = df['date'].dt.month
    df['year'] = df['date'].dt.year
    return df


def prepare_multioutput_data(df, n_steps=8, gap=1):
    """
    PrÃ©pare les donnÃ©es pour la prÃ©diction multi-pÃ©riodes
    Args:
        n_steps: horizon de prÃ©diction (8 semaines)
        gap: dÃ©lai avant premiÃ¨re prÃ©diction (1 semaine)
    """
    # Groupement hebdomadaire
    weekly_sales = df.set_index('date').groupby(
        ['store_nbr', 'family']
    )['sales'].resample('W').sum().unstack(level=[0,1])
    
    # CrÃ©ation des sÃ©quences
    X, y = [], []
    for i in range(len(weekly_sales) - n_steps - gap):
        X.append(weekly_sales.iloc[i])
        y.append(weekly_sales.iloc[i+gap:i+gap+n_steps].values.flatten())
    
    # Conversion en array numpy
    X_array = np.array(X)
    y_array = np.array(y)
    
    # VÃ©rification des dimensions
    if X_array.shape[0] != y_array.shape[0]:
        raise ValueError(f"Dimension mismatch: X has {X_array.shape[0]} samples, y has {y_array.shape[0]}")
    
    # CrÃ©ation des noms de colonnes
    n_features = X_array.shape[1]
    n_targets = y_array.shape[1]
    
    feature_cols = [f"lag_{i}" for i in range(n_features)]
    target_cols = [f"target_w{i}" for i in range(n_targets)]
    
    # CrÃ©ation du DataFrame final
    data = np.concatenate([X_array, y_array], axis=1)
    columns = feature_cols + target_cols
    
    return pd.DataFrame(data=data, columns=columns)



def modeling_page():
    st.title("ğŸ¤– ModÃ©lisation Multi-Sorties")
    
    if 'processed_data' not in st.session_state:
        st.error("Veuillez d'abord prÃ©parer les donnÃ©es dans l'onglet PrÃ©traitement")
        return
    
    data = st.session_state.processed_data
    n_features = len([col for col in data.columns if col.startswith('lag_')])
    n_targets = len([col for col in data.columns if col.startswith('target_')])
    
    # SÃ©paration des features et targets
    X = data[[col for col in data.columns if col.startswith('lag_')]].values
    y = data[[col for col in data.columns if col.startswith('target_')]].values
    
    # Normalisation
    scaler = MinMaxScaler()
    X_scaled = scaler.fit_transform(X)
    y_scaled = scaler.fit_transform(y)
    
    # Configuration du modÃ¨le
    st.sidebar.header("Configuration du ModÃ¨le")
    lstm_units = st.sidebar.slider("UnitÃ©s LSTM", 32, 256, 128)
    dropout_rate = st.sidebar.slider("Taux de Dropout", 0.0, 0.5, 0.2)
    learning_rate = st.sidebar.slider("Taux d'apprentissage", 0.0001, 0.01, 0.001)
    epochs = st.sidebar.slider("Nombre d'Ã©poques", 10, 100, 50)
    batch_size = st.sidebar.slider("Taille de batch", 16, 128, 32)
    
    # Architecture du modÃ¨le
    model = Sequential([
        LSTM(lstm_units, input_shape=(1, n_features)),
        Dropout(dropout_rate),
        Dense(n_targets)
    ])
    
    model.compile(  # <-- Cette ligne doit Ãªtre alignÃ©e avec 'model ='
        optimizer=Adam(learning_rate=learning_rate),
        loss='mse',
        metrics=['mae']
    )
    
    
    # DÃ©coupage temporel
    tscv = TimeSeriesSplit(n_splits=5)
    
    if st.button("Lancer l'entraÃ®nement"):
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        # EntraÃ®nement avec validation croisÃ©e
        for fold, (train_idx, val_idx) in enumerate(tscv.split(X_scaled)):
            X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]
            y_train, y_val = y_scaled[train_idx], y_scaled[val_idx]
            
            # Remodelage pour LSTM [samples, timesteps, features]
            X_train = X_train.reshape(-1, 1, n_features)
            X_val = X_val.reshape(-1, 1, n_features)
            
            history = model.fit(
                X_train, y_train,
                validation_data=(X_val, y_val),
                epochs=epochs,
                batch_size=batch_size,
                verbose=0
            )
            
            # Mise Ã  jour de la progression
            progress = (fold + 1) / tscv.n_splits
            progress_bar.progress(progress)
            status_text.text(f"Fold {fold + 1}/{tscv.n_splits} - Val MAE: {history.history['val_mae'][-1]:.4f}")
        
        # Sauvegarde du modÃ¨le
        st.session_state.model = model
        st.session_state.scaler = scaler
        
        # Visualisation des rÃ©sultats
        plot_training_results(history)
        st.success("ModÃ¨le entraÃ®nÃ© avec succÃ¨s!")

def plot_training_results(history):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    
    # Courbe de loss
    ax1.plot(history.history['loss'], label='Train Loss')
    ax1.plot(history.history['val_loss'], label='Validation Loss')
    ax1.set_title('Ã‰volution de la Loss')
    ax1.set_ylabel('MSE')
    ax1.set_xlabel('Epoch')
    ax1.legend()
    
    # Courbe de MAE
    ax2.plot(history.history['mae'], label='Train MAE')
    ax2.plot(history.history['val_mae'], label='Validation MAE')
    ax2.set_title('Ã‰volution du MAE')
    ax2.set_ylabel('MAE')
    ax2.set_xlabel('Epoch')
    ax2.legend()
    
    st.pyplot(fig)



def evaluation_page():
    st.title("ğŸ“ˆ Post-traitement & Ã‰valuation")
    st.warning("Section en construction - Disponible prochainement")
    # Placeholder pour les fonctions d'Ã©valuation

def main():
    st.sidebar.title("Navigation")
    page = st.sidebar.radio("", [
        "Ã€ Propos de Moi",
        "Exploration des DonnÃ©es (EDA)", 
        "Feature Engineering",
        "PrÃ©traitement",
        "ModÃ©lisation",
        "Post-traitement & Ã‰valuation"
    ])  # <-- Ce crochet ferme bien la liste
    
    try:
        # Chargement conditionnel des donnÃ©es
        if page in ["Exploration des DonnÃ©es (EDA)", "Feature Engineering", "PrÃ©traitement"]:
            data = load_data()
        else:
            data = None
    except Exception as e:
        st.error(f"Erreur de chargement: {str(e)}")
        return
    
    # Router vers la page sÃ©lectionnÃ©e
    if page == "Ã€ Propos de Moi":
        about_page()
    elif page == "Exploration des DonnÃ©es (EDA)":
        eda_page(data)
    elif page == "Feature Engineering":
        feature_engineering_page(data)
    elif page == "PrÃ©traitement":
        preprocessing_page(data)
    elif page == "ModÃ©lisation":
        modeling_page()  # Sans paramÃ¨tre data
    elif page == "Post-traitement & Ã‰valuation":
        evaluation_page()

if __name__ == "__main__":
    main()
